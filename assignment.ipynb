{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc41d19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/TALC/enel645_2025w'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9e362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garbage_data',\n",
       " 'local_dataset',\n",
       " 'transfer_learning',\n",
       " 'slurm-35541.out',\n",
       " 'best_model.pth',\n",
       " 'CombinedModel.py',\n",
       " '.git',\n",
       " 'cm1.slurm',\n",
       " 'cm1.py',\n",
       " 'jupyter_env',\n",
       " 'Assignment2.slurm',\n",
       " '.ipynb_checkpoints',\n",
       " 'test.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3704a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/work/TALC/enel645_2025w/garbage_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5069d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVPR_2024_dataset_Train',\n",
       " 'CVPR_2024_dataset_Test',\n",
       " 'CVPR_2024_dataset_Val',\n",
       " 'Downloads',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data',\n",
       " 'import os.py',\n",
       " 'source',\n",
       " 'garbage_data',\n",
       " 'train.py',\n",
       " 'Sampled_Train',\n",
       " 'local_garbage_data',\n",
       " 'Sampled_Val',\n",
       " 'Sampled_Test',\n",
       " 'garbage_data.tar.gz',\n",
       " 'my_venv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22bbb60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "     |████████████████████████████████| 23.3 MB 2.7 MB/s            \n",
      "\u001b[?25hCollecting torch==1.10.1\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 9.8 kB/s             \n",
      "\u001b[?25hCollecting pillow!=8.3.0,>=5.3.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 50.4 MB/s            \n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "     |████████████████████████████████| 14.8 MB 34.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from torch==1.10.1->torchvision) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from torch==1.10.1->torchvision) (0.8)\n",
      "Installing collected packages: torch, pillow, numpy, torchvision\n",
      "Successfully installed numpy-1.19.5 pillow-8.4.0 torch-1.10.1 torchvision-0.11.2\n",
      "torch.Size([3, 224, 224]) 0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor()           # Convert to Tensor\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\"\n",
    "train_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check a sample\n",
    "image, label = train_dataset[0]\n",
    "print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c534347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "     |████████████████████████████████| 22.2 MB 2.8 MB/s            \n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "     |████████████████████████████████| 25.9 MB 49.4 MB/s            \n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "     |████████████████████████████████| 309 kB 50.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.1 scikit-learn-0.24.2 scipy-1.5.4 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05dcf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "     |████████████████████████████████| 9.5 MB 3.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.17.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdaaad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d6ecdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVPR_2024_dataset_Train',\n",
       " 'CVPR_2024_dataset_Test',\n",
       " 'CVPR_2024_dataset_Val',\n",
       " 'Downloads',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data',\n",
       " 'import os.py',\n",
       " 'source',\n",
       " 'garbage_data',\n",
       " 'train.py',\n",
       " 'Sampled_Train',\n",
       " 'local_garbage_data',\n",
       " 'Sampled_Val',\n",
       " 'Sampled_Test',\n",
       " 'garbage_data.tar.gz',\n",
       " 'my_venv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ecec684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Black': 0, 'Blue': 1, 'Green': 2, 'TTR': 3}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=\"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Val\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Test\", transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check class mapping\n",
    "print(train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a264b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=6):  # Adjust `num_classes` based on your dataset\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.cnn(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ba021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/mdtahmid.jamil/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.9959615393654331, Accuracy=0.60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GarbageClassifier(num_classes=len(train_dataset.classes)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss={running_loss / len(train_loader)}, Accuracy={train_acc:.2f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe31837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization and matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_errors(model, dataloader):\n",
    "    model.eval()\n",
    "    incorrect_samples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for img, pred, true in zip(images, preds, labels):\n",
    "                if pred != true:\n",
    "                    incorrect_samples.append((img.cpu(), pred.cpu().item(), true.cpu().item()))\n",
    "\n",
    "    # Show 5 incorrect samples\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i, (img, pred, true) in enumerate(incorrect_samples[:5]):\n",
    "        axs[i].imshow(img.permute(1, 2, 0))\n",
    "        axs[i].set_title(f\"Pred: {pred}, True: {true}\")\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_errors(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the train model\n",
    "torch.save(model.state_dict(), \"garbage_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
