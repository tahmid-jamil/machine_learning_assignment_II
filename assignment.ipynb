{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3daf01ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/TALC/enel645_2025w'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cee4471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garbage_data',\n",
       " 'local_dataset',\n",
       " 'transfer_learning',\n",
       " 'slurm-35541.out',\n",
       " 'best_model.pth',\n",
       " 'CombinedModel.py',\n",
       " '.git',\n",
       " 'cm1.slurm',\n",
       " 'cm1.py',\n",
       " 'jupyter_env',\n",
       " 'Assignment2.slurm',\n",
       " '.ipynb_checkpoints',\n",
       " 'test.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddebc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/work/TALC/enel645_2025w/garbage_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e967d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVPR_2024_dataset_Train',\n",
       " 'CVPR_2024_dataset_Test',\n",
       " 'CVPR_2024_dataset_Val',\n",
       " 'Downloads',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data',\n",
       " 'import os.py',\n",
       " 'source',\n",
       " 'garbage_data',\n",
       " 'train.py',\n",
       " 'Sampled_Train',\n",
       " 'local_garbage_data',\n",
       " 'Sampled_Val',\n",
       " 'Sampled_Test',\n",
       " 'garbage_data.tar.gz',\n",
       " 'my_venv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6737fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "     |████████████████████████████████| 23.3 MB 2.7 MB/s            \n",
      "\u001b[?25hCollecting torch==1.10.1\n",
      "  Downloading torch-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 9.8 kB/s             \n",
      "\u001b[?25hCollecting pillow!=8.3.0,>=5.3.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 50.4 MB/s            \n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "     |████████████████████████████████| 14.8 MB 34.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from torch==1.10.1->torchvision) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from torch==1.10.1->torchvision) (0.8)\n",
      "Installing collected packages: torch, pillow, numpy, torchvision\n",
      "Successfully installed numpy-1.19.5 pillow-8.4.0 torch-1.10.1 torchvision-0.11.2\n",
      "torch.Size([3, 224, 224]) 0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor()           # Convert to Tensor\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\"\n",
    "train_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check a sample\n",
    "image, label = train_dataset[0]\n",
    "print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e223a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "     |████████████████████████████████| 22.2 MB 2.8 MB/s            \n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "     |████████████████████████████████| 25.9 MB 49.4 MB/s            \n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "     |████████████████████████████████| 309 kB 50.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.1.1 scikit-learn-0.24.2 scipy-1.5.4 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5796b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "     |████████████████████████████████| 9.5 MB 3.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /work/TALC/enel645_2025w/jupyter_env/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.17.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41392b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train/train_labels.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-431e50b5859f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/work/TALC/enel645_2025w/garbage_data'\u001b[0m \u001b[0;31m# Replace with your actual data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarbageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, text_transform=text_transform) # Add text_transform if you define one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarbageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, text_transform=text_transform)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarbageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, text_transform=text_transform)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-431e50b5859f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, split, transform, text_transform)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# --- Load Data ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Assuming format \"filename label\" in txt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train/train_labels.txt'"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "#import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image # To handle images\n",
    "\n",
    "# --- Define Dataset Class ---\n",
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None, text_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the data (train, val, test folders).\n",
    "            split (string): 'train', 'val', or 'test'.\n",
    "            transform (callable, optional): Optional transform to be applied on image.\n",
    "            text_transform (callable, optional): Optional transform to be applied on text data (if applicable).\n",
    "        \"\"\"\n",
    "        self.data_dir = os.path.join(data_dir, f'CVPR_2024_dataset_{split.capitalize()}') # Assuming folder structure\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.text_transform = text_transform\n",
    "        self.image_dir = os.path.join(self.data_dir, 'images')\n",
    "        self.text_file = os.path.join(self.data_dir, f'{split}_labels.txt') # Assuming labels are in a text file\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.labels = []\n",
    "        self.texts = [] # Placeholder for text data if available\n",
    "\n",
    "        # --- Load Data ---\n",
    "        with open(self.text_file, 'r') as f:\n",
    "            for line in f:\n",
    "                filename, label = line.strip().split() # Assuming format \"filename label\" in txt file\n",
    "                self.image_filenames.append(filename)\n",
    "                self.labels.append(int(label)) # Assuming labels are integers. Adjust if needed.\n",
    "                # --- Load Textual Data (Example - Adapt based on your actual text data format) ---\n",
    "                # For example, if you have text descriptions in separate files named like image filenames\n",
    "                text_filepath = os.path.join(self.data_dir, 'texts', filename.replace('.jpg', '.txt')) # Example path\n",
    "                if os.path.exists(text_filepath):\n",
    "                    with open(text_filepath, 'r', encoding='utf-8') as text_f: # Handle encoding if necessary\n",
    "                        text_content = text_f.read().strip()\n",
    "                        self.texts.append(text_content)\n",
    "                else:\n",
    "                    self.texts.append(\"\") # Or handle missing text as needed\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert('RGB') # Ensure RGB for consistency\n",
    "        label = self.labels[idx]\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # if self.text_transform: # Apply text transform if needed (e.g., tokenization, embedding)\n",
    "        #     text = self.text_transform(text) # Placeholder - Implement text transform if necessary\n",
    "\n",
    "        return image, text, label # Return image, text, and label\n",
    "\n",
    "\n",
    "# --- Define Data Transformations ---\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), # Or other appropriate size\n",
    "    transforms.RandomHorizontalFlip(), # Data augmentation - adjust as needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization - adjust if needed\n",
    "])\n",
    "\n",
    "# --- Text Transformation (Placeholder - Adapt based on your text data and model) ---\n",
    "# Example using simple tokenization (you might need more advanced techniques like BERT, etc.)\n",
    "# def simple_tokenizer(text):\n",
    "#     return text.split()\n",
    "# text_transform = simple_tokenizer # Example, replace with your actual text processing\n",
    "\n",
    "\n",
    "# --- Instantiate Datasets ---\n",
    "data_dir = '/work/TALC/enel645_2025w/garbage_data' # Replace with your actual data directory\n",
    "\n",
    "train_dataset = GarbageDataset(data_dir, split='train', transform=image_transform) #, text_transform=text_transform) # Add text_transform if you define one\n",
    "val_dataset = GarbageDataset(data_dir, split='val', transform=image_transform) #, text_transform=text_transform)\n",
    "test_dataset = GarbageDataset(data_dir, split='test', transform=image_transform) #, text_transform=text_transform)\n",
    "\n",
    "\n",
    "# --- Data Loaders ---\n",
    "batch_size = 32 # Adjust batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4) # Adjust num_workers\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# --- Define Model Architecture ---\n",
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        # --- Image Feature Extractor (Example - ResNet18) ---\n",
    "        self.image_encoder = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) # Or other CNN\n",
    "        # Remove last layers for feature extraction\n",
    "        self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-2]) # Remove avgpool and fc\n",
    "\n",
    "        # --- Text Feature Extractor (Placeholder - Example: Simple Embedding and RNN) ---\n",
    "        # You'll need to adapt this based on your text data and desired complexity\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim) # Define vocab_size and embedding_dim\n",
    "        # self.text_rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True) # Define hidden_size\n",
    "\n",
    "        # --- Fusion Layer (Example - Concatenation and Linear layer) ---\n",
    "        # self.fusion_layer = nn.Linear(image_feature_size + text_feature_size, fusion_output_size) # Define sizes\n",
    "        self.fusion_layer = nn.Linear(512, 256) # Example ResNet18 feature size is 512, adjust if needed, remove text part for now\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.classifier = nn.Linear(256, num_classes) # From fusion output to number of classes\n",
    "\n",
    "\n",
    "    def forward(self, images, texts): # Expects image and text batches\n",
    "        # --- Image Feature Extraction ---\n",
    "        image_features = self.image_encoder(images) # [batch_size, C, H, W]\n",
    "        image_features = torch.mean(image_features, dim=(-2, -1)) # Global Average Pooling [batch_size, C]\n",
    "\n",
    "\n",
    "        # --- Text Feature Extraction (Placeholder - Adapt based on your text model) ---\n",
    "        # text_embedded = self.embedding(texts) # [batch_size, seq_len, embedding_dim]\n",
    "        # text_features, _ = self.text_rnn(text_embedded) # [batch_size, seq_len, hidden_size]\n",
    "        # text_features = text_features[:, -1, 🙂 # Take last time step's output [batch_size, hidden_size] # Or use attention, etc.\n",
    "\n",
    "        # --- Feature Fusion (Example - Concatenation) ---\n",
    "        # combined_features = torch.cat((image_features, text_features), dim=1) # Concatenate image and text features\n",
    "        combined_features = image_features # Removing text for now\n",
    "\n",
    "        fused_features = self.fusion_layer(combined_features)\n",
    "        fused_features = torch.relu(fused_features) # Activation function\n",
    "\n",
    "        # --- Classification ---\n",
    "        output = self.classifier(fused_features)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# --- Instantiate Model, Loss, Optimizer ---\n",
    "num_classes = 6 # Example number of garbage classes - adjust based on your dataset\n",
    "model = GarbageClassifier(num_classes=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Common loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Common optimizer, adjust learning rate as needed\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use GPU if available\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 10 # Adjust number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, texts, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        # texts = texts.to(device) # If you process text as tensors - adapt text processing\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts) # Pass both image and text to the model\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad(): # Disable gradient calculation during validation\n",
    "        for images, texts, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            # texts = texts.to(device) # If you process text as tensors\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images, texts) # Pass both image and text\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {(correct_train/total_train)*100:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {(correct_val/total_val)*100:.2f}%')\n",
    "\n",
    "\n",
    "# --- Evaluation on Test Set ---\n",
    "model.eval() # Ensure model is in evaluation mode\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, texts, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        # texts = texts.to(device) # If you process text as tensors\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy()) # Store predictions for metrics\n",
    "        all_labels.extend(labels.cpu().numpy()) # Store true labels for metrics\n",
    "\n",
    "\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {(correct_test/total_test)*100:.2f}%')\n",
    "\n",
    "# --- Classification Report and Confusion Matrix ---\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "\n",
    "# --- Save Predictions/Incorrect Classifications (Example - adapt as needed) ---\n",
    "incorrect_indices = np.where(np.array(all_preds) != np.array(all_labels))[0]\n",
    "incorrect_filenames = [test_dataset.image_filenames[i] for i in incorrect_indices]\n",
    "incorrect_predictions = [all_preds[i] for i in incorrect_indices]\n",
    "incorrect_true_labels = [all_labels[i] for i in incorrect_indices]\n",
    "\n",
    "incorrect_df = pd.DataFrame({\n",
    "    'filename': incorrect_filenames,\n",
    "    'predicted_label': incorrect_predictions,\n",
    "    'true_label': incorrect_true_labels\n",
    "})\n",
    "\n",
    "print(\"\\nExamples of Incorrect Classifications:\")\n",
    "print(incorrect_df.head()) # Print first few incorrect examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b438a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of garbage_data directory:\n",
      "['CVPR_2024_dataset_Train', 'CVPR_2024_dataset_Test', 'CVPR_2024_dataset_Val', 'Downloads', '.ipynb_checkpoints', 'Data', 'import os.py', 'source', 'garbage_data', 'train.py', 'Sampled_Train', 'local_garbage_data', 'Sampled_Val', 'Sampled_Test', 'garbage_data.tar.gz', 'my_venv']\n",
      "\n",
      "Contents of CVPR_2024_dataset_Train directory:\n",
      "['Blue', 'TTR', 'Green', 'Black']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_root = '/work/TALC/enel645_2025w/garbage_data'\n",
    "\n",
    "print(\"Contents of garbage_data directory:\")\n",
    "print(os.listdir(data_root)) # List the folders inside garbage_data\n",
    "\n",
    "train_dir = os.path.join(data_root, 'CVPR_2024_dataset_Train') # Construct the train directory path\n",
    "print(\"\\nContents of CVPR_2024_dataset_Train directory:\")\n",
    "if os.path.exists(train_dir): # Check if the directory exists before listing\n",
    "    print(os.listdir(train_dir))\n",
    "else:\n",
    "    print(f\"Directory '{train_dir}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf35592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None, text_transform=None):\n",
    "        # ... (rest of the init) ...\n",
    "\n",
    "        if split == 'train':\n",
    "            split_folder = 'train_data'\n",
    "        elif split == 'val':\n",
    "            split_folder = 'val_data'\n",
    "        elif split == 'test':\n",
    "            split_folder = 'test_data'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split. Choose 'train', 'val', or 'test'.\")\n",
    "\n",
    "        self.data_dir = os.path.join(data_dir, split_folder) # Use split_folder\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.text_transform = text_transform\n",
    "        self.image_dir = os.path.join(self.data_dir, 'images')\n",
    "        self.text_file = os.path.join(self.data_dir, 'labels.csv') # Filename is labels.csv now\n",
    "\n",
    "        # ... (rest of the init, adjust label loading for CSV if needed) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3cbb5f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4d9f43b55895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'CVPR_2024_dataset_{split.capitalize()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split' is not defined"
     ]
    }
   ],
   "source": [
    "self.data_dir = os.path.join(data_dir, f'CVPR_2024_dataset_{split.capitalize()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a2d9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e470095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVPR_2024_dataset_Train',\n",
       " 'CVPR_2024_dataset_Test',\n",
       " 'CVPR_2024_dataset_Val',\n",
       " 'Downloads',\n",
       " '.ipynb_checkpoints',\n",
       " 'Data',\n",
       " 'import os.py',\n",
       " 'source',\n",
       " 'garbage_data',\n",
       " 'train.py',\n",
       " 'Sampled_Train',\n",
       " 'local_garbage_data',\n",
       " 'Sampled_Val',\n",
       " 'Sampled_Test',\n",
       " 'garbage_data.tar.gz',\n",
       " 'my_venv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d928d0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8e85a4122e28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m ])\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarbageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8e85a4122e28>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, root_dir, transform)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGarbageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/TALC/enel645_2025w/jupyter_env/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/TALC/enel645_2025w/jupyter_env/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/TALC/enel645_2025w/jupyter_env/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/TALC/enel645_2025w/jupyter_env/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/TALC/enel645_2025w/jupyter_env/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "class GarbageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        text = self.data.iloc[idx, 1]  # Assuming the second column is textual data\n",
    "        label = int(self.data.iloc[idx, 2])  # Assuming the third column is the label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, text, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = GarbageDataset(\"train.csv\", \"train_images\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a73fffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Black': 0, 'Blue': 1, 'Green': 2, 'TTR': 3}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=\"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Val\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"/work/TALC/enel645_2025w/garbage_data/CVPR_2024_dataset_Test\", transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check class mapping\n",
    "print(train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a75f77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=6):  # Adjust `num_classes` based on your dataset\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.cnn(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e0d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/mdtahmid.jamil/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GarbageClassifier(num_classes=len(train_dataset.classes)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss={running_loss / len(train_loader)}, Accuracy={train_acc:.2f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
